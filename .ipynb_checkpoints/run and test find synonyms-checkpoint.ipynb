{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5862ac5b-e819-4cca-b5d6-d56c84486f11",
   "metadata": {},
   "source": [
    "# Find Synonyms project\n",
    "- Anna Chen\n",
    "- October 2024  \n",
    "#### This program processes text data and calculates semantic similarity between words based on their co-occurrence in sentences, using a bag-of-words model and cosine similarity.\n",
    "\n",
    "Key features include:<br>\n",
    "    - Text preprocessing: expanding contractions, stemming, lemmatization, and removing stop words.<br>\n",
    "    - Building semantic descriptors: creating word co-occurrence contexts.<br>\n",
    "    - Calculating cosine similarity: measuring word relations.<br>\n",
    "    - Running similarity tests: checking program accuracy using a test file.\n",
    "\n",
    "synonyms.py builds on \"Semantic Similarity\" starter code.<br>\n",
    "\n",
    "Starter Code<br>\n",
    "Original Author: Michael Guerzhoy, University of Toronto, October 2014.<br>\n",
    "Modified with permission by Marcus Gubanyi, Concordia University-Nebraska, October 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60c90a9-a4ab-44fa-baae-4ad9ec29a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s9602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s9602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s9602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Find synonym program started...\n",
      "\n",
      "Finished getting words and sentences from file (training data)\n",
      "----------------\n",
      "Finished getting word_context\n",
      "    ======\n",
      "test file: test.txt\n",
      "The percentage of correct guesses is 0.0 %\n",
      "The training data used: ['Swann’s Way by Marcel Proust.txt']\n",
      "----------------\n",
      "----------------\n",
      "Find synonym program started...\n",
      "\n",
      "Finished getting words and sentences from file (training data)\n",
      "----------------\n",
      "Finished getting word_context\n",
      "    ======\n",
      "test file: test.txt\n",
      "The percentage of correct guesses is 0.0 %\n",
      "The training data used: ['War and Peace by Leo Tolstoy.txt']\n",
      "----------------\n",
      "\n",
      "============== ⬇️⬇️ Use the altered test file ⬇️⬇️ ============================\n",
      "\n",
      "----------------\n",
      "Find synonym program started...\n",
      "\n",
      "Finished getting words and sentences from file (training data)\n",
      "----------------\n",
      "Finished getting word_context\n",
      "    ======\n",
      "test file: test altered.txt\n",
      "The percentage of correct guesses is 25.0 %\n",
      "The training data used: ['Swann’s Way by Marcel Proust.txt']\n",
      "----------------\n",
      "----------------\n",
      "Find synonym program started...\n",
      "\n",
      "Finished getting words and sentences from file (training data)\n",
      "----------------\n",
      "Finished getting word_context\n",
      "    ======\n",
      "test file: test altered.txt\n",
      "The percentage of correct guesses is 37.5 %\n",
      "The training data used: ['War and Peace by Leo Tolstoy.txt']\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# run the program \n",
    "%run synonyms.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11114909-45ab-4e9a-a7a0-8eb4490cc9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s9602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s9602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s9602\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the find synonyms program function to test\n",
    "from synonyms import expand_contractions, get_sentence_lists, get_sentence_lists_from_files\n",
    "from synonyms import build_semantic_descriptors, most_similar_word, run_similarity_test, run_program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7cd1e-2a0c-4241-b818-98779e37a285",
   "metadata": {},
   "source": [
    "### Test the functions I wrote to varify it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33352d87-c2ed-4f7e-b3c0-b9c7bdd08cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:I don't think it's possible. He hasn't decided yet.\n",
      "Text after expand contraction: I do not think it's possible. He has not decided yet.\n"
     ]
    }
   ],
   "source": [
    "def test_expand_contractions():\n",
    "    text = \"I don't think it's possible. He hasn't decided yet.\"\n",
    "    print(\"Text:\" + text)\n",
    "    expanded_text = expand_contractions(text, contractions)\n",
    "    print(f\"Text after expand contraction: {expanded_text}\")\n",
    "    expected_output = \"I do not think it is possible. He has not decided yet.\"\n",
    "\n",
    "test_expand_contractions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd35df3-ad5b-4b06-b686-58fa6c0a0f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I don't agree with that. However, it's his decision.\n",
      "result: [['not', 'agre'], ['howev', 'decis']]\n"
     ]
    }
   ],
   "source": [
    "def test_get_sentence_lists():\n",
    "    text = \"I don't agree with that. However, it's his decision.\"\n",
    "    print(\"Text: \" + text)\n",
    "    \n",
    "    sentence_lists = get_sentence_lists(text)\n",
    "    print(f\"result: {sentence_lists}\")\n",
    "\n",
    "    \n",
    "test_get_sentence_lists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e2155b-738f-44c8-803c-704c2873ecb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file names: ['test 2.txt', 'test 3.txt', 'test n.txt']\n",
      "\n",
      "Error: File test n.txt not found.\n",
      "\n",
      "result: [['test', 'file', 'hope', 'python', 'function', 'work'], ['today', 'work', 'dicid', 'learn', 'python'], ['actual', 'review', 'python'], ['like', 'cat', 'dog'], ['think', 'dog', 'better', 'easier', 'keep', 'healthi'], ['not', 'agre', 'either'], ['like', 'plant', 'snake'], ['dragon', 'also', 'cool', 'expens', 'keep'], ['cannot', 'one', 'even', 'abl', 'find', 'wild', 'dragon']]\n"
     ]
    }
   ],
   "source": [
    "def test_get_sentence_lists_from_files():\n",
    "\n",
    "    # test n.txt is to check how non-existing file is handled\n",
    "    filenames = [\"test 2.txt\", \"test 3.txt\", \"test n.txt\"]\n",
    "    print(f\"file names: {filenames}\\n\")\n",
    "    \n",
    "    all_sentences = get_sentence_lists_from_files(filenames)\n",
    "    print(f\"\\nresult: {all_sentences}\")\n",
    "\n",
    "test_get_sentence_lists_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a4db27-191e-4101-bea8-4d740a56c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: [['cat', 'dog'], ['cat', 'fish'], ['dog', 'fish']]\n",
      "\n",
      "word context: {'dog': {'cat': 1, 'fish': 1}, 'cat': {'dog': 1, 'fish': 1}, 'fish': {'cat': 1, 'dog': 1}}\n"
     ]
    }
   ],
   "source": [
    "def test_build_semantic_descriptors():\n",
    "    \n",
    "    sentences = [['cat', 'dog'], ['cat', 'fish'], ['dog', 'fish']]\n",
    "    print(f\"sentences: {sentences}\\n\")\n",
    "    \n",
    "    descriptors = build_semantic_descriptors(sentences)\n",
    "    print(f\"word context: {descriptors}\")\n",
    "    \n",
    "test_build_semantic_descriptors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de88c35-89c0-4e3f-858a-d83d3bf93d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best choice picked: dog\n"
     ]
    }
   ],
   "source": [
    "def test_most_similar_word():\n",
    "    descriptors = {\n",
    "        'feline': {'cat': 3, 'lion': 1, 'pet': 1},\n",
    "        'cat': {'feline': 3, 'lion': 1, 'pet': 2},\n",
    "        'dog': {'pet': 1},\n",
    "        'horse': {'animal': 2}\n",
    "    }\n",
    "    word = 'feline'\n",
    "    choices = ['cat', 'dog', 'horse']\n",
    "    best_choice = most_similar_word(word, choices, descriptors)\n",
    "    print(f\"the best choice picked: {best_choice}\")\n",
    "    \n",
    "test_most_similar_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821a669-4349-4864-92dd-1bdbfd6e4a8a",
   "metadata": {},
   "source": [
    "def test_run_program():\n",
    "    # train with Swann’s Way\n",
    "    run_program([\"Swann’s Way by Marcel Proust.txt\"], \"test.txt\")\n",
    "\n",
    "    # train with War and Peace\n",
    "    run_program([\"War and Peace by Leo Tolstoy.txt\"], \"test.txt\")\n",
    "\n",
    "    # train with both\n",
    "    run_program([\"Swann’s Way by Marcel Proust.txt\", \"War and Peace by Leo Tolstoy.txt\"], \"test.txt\")\n",
    "\n",
    "    print(\"\\n============== ⬇️⬇️ Use the altered test file ⬇️⬇️ ============================\\n\")\n",
    "    # I altered the test.txt file to make the format fit the description \n",
    "    # on the assignment instruction.\n",
    "\n",
    "    # train with Swann’s Way\n",
    "    run_program([\"Swann’s Way by Marcel Proust.txt\"], \"test altered.txt\")\n",
    "\n",
    "    # train with War and Peace\n",
    "    run_program([\"War and Peace by Leo Tolstoy.txt\"], \"test altered.txt\")\n",
    "\n",
    "\n",
    "test_run_program()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43d81b-2bf8-436c-b248-27f42174f07d",
   "metadata": {},
   "source": [
    "###　Observation and Anzltsis\n",
    "The model trained on *War and Peace* outperformed the one trained on *Swann's Way*. My assumption is that *War and Peace* provided a significantly larger dataset—its text file is approximately three times the size of *Swann’s Way*. Generally, larger datasets improve model performance by allowing the model to learn from more varied contexts.\n",
    "\n",
    "However, even with this improvement, the model’s accuracy remains low, correctly identifying synonyms only 37.5% of the time. This level of performance is far from ideal. I believe that substantial improvements could be achieved if the model were trained on a much larger and more diverse dataset, which would allow it to capture word relationships more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692f098-c837-4f52-a4cd-eed7d6e355f1",
   "metadata": {},
   "source": [
    "#### Note on AI usage\n",
    "\n",
    "Most of the code in synonyms.py was written with help from ChatGPT. I worked back and forth with it to improve the code, making sure I understood each part. I added comments and docstrings to explain what the code does and wrote the run_program and main functions myself. \n",
    "\n",
    "For the testing part, ChatGPT gave me a basic version, and I used it as inspiration to come up with the final version. The longer paragraphs in the documentation were first written by me, then I used ChatGPT to help with grammar and flow to make everything easier to read."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
